{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CISSM Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup.\n",
    "Estas librerías proporcionan la base para realizar el procesamiento y análisis de datos de forma eficiente y visualmente comprensible. Su uso combinado permite realizar tareas desde la manipulación de datos hasta su visualización de manera fluida y efectiva en el entorno de Jupyter Notebook.\n",
    "1. **Pandas**: Librería de Python especializada en el manejo y análisis de datos tabulares. Permite realizar operaciones de transformación, agregación, limpieza y análisis de datos de manera eficiente.\n",
    "\n",
    "2. **Numpy**: Librería fundamental para la computación científica en Python. Ofrece soporte para arrays multidimensionales, matrices y funciones matemáticas avanzadas.\n",
    "\n",
    "3. **IPython.display**: Funciones útiles para mostrar contenido interactivo y enriquecido en los notebooks. Con estas funciones, se puede renderizar HTML, mostrar tablas y gráficos de manera directa en el cuaderno de Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from time import sleep\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Process: Extract, Transform y Load (ETL).\n",
    "El pipeline de procesamiento de datos se estructura en tres pasos principales: **Extract**, **Transform** y **Load**. Este flujo se automatiza y gestiona utilizando **Azure Data Factory** (ADF) y se complementa con **Azure Databricks** para realizar el procesamiento y transformación de datos de manera eficiente.\n",
    "\n",
    "1. La primera fase del pipeline consiste en la **extracción** de datos desde diversas fuentes. Estas fuentes pueden incluir bases de datos, archivos en la nube o sistemas externos.\n",
    "\n",
    "2. Una vez que los datos han sido extraídos, el siguiente paso es la **transformación**. Este paso incluye la limpieza, agregación y modificación de los datos para que sean adecuados para el análisis.\n",
    "\n",
    "3. Finalmente, después de que los datos han sido transformados, llega el paso de la **carga**. Este paso implica mover los datos procesados a su destino final, que puede ser una base de datos, un sistema de almacenamiento o un sistema de análisis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **01. Extract.**\n",
    "- **Azure Data Factory** gestiona la conexión a las fuentes de datos.\n",
    "- Se pueden programar procesos de extracción de manera recurrente, asegurando que siempre se tenga acceso a los datos más recientes.\n",
    "- La extracción de datos se puede automatizar con **triggers** o eventos programados para iniciar el flujo de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DATE = '2024-04-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate row intervals.\n",
    "Este código divide un rango de filas (hasta 600) en intervalos de 100 filas. Crea un DataFrame con dos columnas, `start_row` y `end_row`, que representan los límites de cada intervalo. Esto permite procesar los datos en bloques más pequeños. Finalmente, convierte los valores de las columnas a cadenas de texto y muestra los primeros intervalos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_row</th>\n",
       "      <th>end_row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>400</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  start_row end_row\n",
       "0         0     100\n",
       "1       100     200\n",
       "2       200     300\n",
       "3       300     400\n",
       "4       400     500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_rows = 600  # Max extraction limit\n",
    "step = 100\n",
    "start_rows = range(0, max_rows, step)\n",
    "end_rows = [min(start_row + step, max_rows) for start_row in start_rows]\n",
    "\n",
    "df_row_intervals = pd.DataFrame({'start_row': start_rows, 'end_row': end_rows})\n",
    "df_row_intervals = df_row_intervals.astype(str)\n",
    "df_row_intervals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate links.\n",
    "Este código genera una lista de URLs, reemplazando los valores de `start_row` y `end_row` en una plantilla de URL para cada intervalo de filas en el DataFrame. Cada URL corresponde a un intervalo de filas específico, y la lista resultante se guarda en la variable `links`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://cissm.liquifiedapps.com/api/?summary=1&start_row=0&end_row=100&filter={}&sort=[]',\n",
       " 'https://cissm.liquifiedapps.com/api/?summary=1&start_row=100&end_row=200&filter={}&sort=[]',\n",
       " 'https://cissm.liquifiedapps.com/api/?summary=1&start_row=200&end_row=300&filter={}&sort=[]',\n",
       " 'https://cissm.liquifiedapps.com/api/?summary=1&start_row=300&end_row=400&filter={}&sort=[]',\n",
       " 'https://cissm.liquifiedapps.com/api/?summary=1&start_row=400&end_row=500&filter={}&sort=[]',\n",
       " 'https://cissm.liquifiedapps.com/api/?summary=1&start_row=500&end_row=600&filter={}&sort=[]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_patt = \"https://cissm.liquifiedapps.com/api/?summary=1&start_row={start_row}&end_row={end_row}&filter={{}}&sort=[]\"\n",
    "links = df_row_intervals.apply(lambda x: url_patt.format(start_row=x['start_row'], end_row=x['end_row']), axis=1).to_list()\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36 Edg/117.0.2045.41',\n",
    "    'Content-Type': 'text/html',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cissm.liquifiedapps.com/api/?summary=1&start_row=0&end_row=100&filter={}&sort=[]\n",
      "Failed to retrieve data from https://cissm.liquifiedapps.com/api/?summary=1&start_row=0&end_row=100&filter={}&sort=[]. HTTP Status Code: 503\n",
      "https://cissm.liquifiedapps.com/api/?summary=1&start_row=100&end_row=200&filter={}&sort=[]\n",
      "Failed to retrieve data from https://cissm.liquifiedapps.com/api/?summary=1&start_row=100&end_row=200&filter={}&sort=[]. HTTP Status Code: 503\n",
      "https://cissm.liquifiedapps.com/api/?summary=1&start_row=200&end_row=300&filter={}&sort=[]\n",
      "Failed to retrieve data from https://cissm.liquifiedapps.com/api/?summary=1&start_row=200&end_row=300&filter={}&sort=[]. HTTP Status Code: 503\n",
      "https://cissm.liquifiedapps.com/api/?summary=1&start_row=300&end_row=400&filter={}&sort=[]\n",
      "Failed to retrieve data from https://cissm.liquifiedapps.com/api/?summary=1&start_row=300&end_row=400&filter={}&sort=[]. HTTP Status Code: 503\n",
      "https://cissm.liquifiedapps.com/api/?summary=1&start_row=400&end_row=500&filter={}&sort=[]\n",
      "Failed to retrieve data from https://cissm.liquifiedapps.com/api/?summary=1&start_row=400&end_row=500&filter={}&sort=[]. HTTP Status Code: 503\n",
      "https://cissm.liquifiedapps.com/api/?summary=1&start_row=500&end_row=600&filter={}&sort=[]\n",
      "Failed to retrieve data from https://cissm.liquifiedapps.com/api/?summary=1&start_row=500&end_row=600&filter={}&sort=[]. HTTP Status Code: 503\n",
      "Saving intermediate result in a temporal JSON file\n"
     ]
    }
   ],
   "source": [
    "incidents_table = []\n",
    "\n",
    "for i, url in enumerate(links):\n",
    "    print(url)\n",
    "    sleep(random.randint(1, 60))\n",
    "    page = requests.get(url, headers=headers)\n",
    "    if page.status_code != 200:\n",
    "        print(f\"Failed to retrieve data from {url}. HTTP Status Code: {page.status_code}\")\n",
    "        continue  # Salta esta iteración y continúa con la siguiente URL\n",
    "    page_content = page.content.decode(\"utf-8\")\n",
    "    if not page_content:\n",
    "        print(f\"Empty response for URL: {url}\")\n",
    "        continue\n",
    "    try:\n",
    "        incidents = json.loads(page_content)['rows']\n",
    "        if incidents == False:\n",
    "            incidents = []\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON for URL: {url}\")\n",
    "        print(f\"Response content: {page_content}\")\n",
    "        incidents = []\n",
    "\n",
    "    incidents_table.extend(incidents)\n",
    "\n",
    "    # Si no hay incidentes, asume que no hay más resultados y detiene el proceso\n",
    "    if len(incidents) == 0:\n",
    "        print(f\"No incidents were returned. URL: {url}\")\n",
    "        print(f\"End of incidents is assumed and process will be stopped.\")\n",
    "        break\n",
    "\n",
    "# Guardar el resultado intermedio en un archivo JSON temporal\n",
    "print(\"Saving intermediate result in a temporal JSON file\")\n",
    "with open('incidents_temp.json', 'w') as f:\n",
    "    json.dump(incidents_table, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
